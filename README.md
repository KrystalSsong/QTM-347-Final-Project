# Predictions of Cohort Loan Default Rates Based on ML Models
Authors: Sigrid Liu, Jenny Zhang, Krystal Song, Christina Li

## Abstract
This project investigates the default rates of student loans within two years of graduation using the "Most-Recent-Cohorts-Institution" dataset. The analysis focuses on variables such as institution type, student demographics, academic factors, and environmental conditions, chosen for their relevance and significant correlation with loan default rates. We employ a combination of, forward selection, and lasso regression for feature selection, alongside decision tree and random forest regressor for predictive modelling. Additionally, classification techniques like decision trees and random forests are used to identify key patterns. The study aims to provide insights useful for policymakers and educational institutions in developing strategies to mitigate loan default risks.

## Introduction
### Problem Statement and Motivation
The problem we are addressing is the investigation of the default rate of student loans within two years of graduation. A student is granted default status if the student fails to repay within 270 days of repayment date. The primary goal is to identify the key factors that influence this default rate, focusing on a range of variables such as the nature of the institution (public or private), student demographics (race, gender), academic factors (highest degree awarded, majors), environmental aspects (student to faculty ratio, part-time or full-time status), and median incomes of students after graduation.

This investigation is driven by the increasing concern over student loan debt and its impact on both individuals and the economy. The topic is especially relevant in today’s market environment. As we are going under economic downturns and multiple hiring freezes across industries, students are struggling to obtain income to pay off their debts, which can leave negative long-term impacts on their credit records. On a broader scale, as more students default, the Federal government will be forced into providing easing strategies, such as student loan forgiveness. However, such easing can lead to higher consumer expenditures as the stress of repayments is alleviated. As a result, the easing of student loans can lead to more inflations, which have been one of the biggest issues faced by the U.S. economy.

Understanding the factors that lead to loan defaults can provide critical insights for policymakers, educational institutions, and financial organisations. This problem is particularly interesting as it intersects with broader issues such as educational equity, economic stability, and social mobility. The insights gained from this study can be instrumental in shaping policies and practices that aim to reduce the financial burden on students and mitigate the risk of loan defaults. Use cases for this research are found in higher education policy formulation, financial counselling for students, and strategic planning by educational institutions.

### Approach and Methodology
To tackle this problem, we propose a multifaceted analytical approach using forward selection and lasso regression for feature selection, supplemented by decision tree and random forest regressor for predictive modelling. Additionally, we employ decision trees and random forest classifiers for classification purposes. This approach is advantageous as it combines the strength of various techniques: decision trees for their interpretability and ability to handle non-linear relationships, lasso regression for its efficacy in feature reduction and handling overfitting, and PLS regression for its ability to deal with multicollinearity in predictor variables. Logistic regression provides a robust framework for predicting categorical outcomes and control overfitting. Decision trees classifier offers a simple and interpretable structure for decision-making, requiring little data preparation and effectively handling both numerical and categorical data, while random forests classifier generates high accuracy by aggregating results from multiple decision trees, thus preventing overfitting and handling unbalanced datasets effectively. Our approach differs from traditional linear methods by incorporating a more nuanced analysis that can capture complex patterns within the data. Key components of our methodology include a comprehensive data preprocessing phase, rigorous feature selection to identify the most influential variables, and a combination of predictive models to capture different aspects of the data. However, this approach has limitations, such as the potential for overfitting in decision trees and the sensitivity of lasso regression to the choice of tuning parameters. Moreover, our models' predictive accuracy is contingent on the representativeness and quality of the dataset used.

## Setup
### Problem Setup
For this project, the main problem setup revolves around predicting the likelihood of cohort student loan defaults based on various institutional and student-related factors. The response variable is the default rate for each institution, and the predictors include institution type, student demographics, academic factors, and others. The data will be split into training and test sets, ensuring a representative sample for model training and validation. The models will be evaluated based on their accuracy and precision, among other metrics, to ensure robustness and reliability in predictions. The experimental setup is designed to provide a comprehensive and in-depth analysis of the factors leading to student loan defaults, offering valuable insights for stakeholders in the education sector.

### Dataset
The dataset Most-Recent-Cohorts-Institution is an extensive compilation of data provided by the U.S. Department of Education. It is highly detailed and multifaceted, and contains 6,543 observations, detailing institutional characteristics, enrollment, student aid, costs, and student outcomes. The scale and diversity of variables provide insights into aspects like student demographics, academic offerings, institutional characteristics, student financials and loan default rates.

### Data Cleaning
We begin by refining our dataset to ensure its relevance and completeness. Firstly, we eliminate columns with fewer than 4000 observations or those marked as 'PrivacySuppressed'. We then focus on observations that are relevant to operating main campus institutions only. Essential attributes such as basic school information, student demographics, academic factors, and default rates are manually selected for further analysis. Observations with significant missing data across most variables are discarded. To deal with remaining missing values, we employ a group-by strategy, finding average values within each group, and calculating percentages where applicable.
The next step involves encoding the data to make it suitable for analysis. Numerical columns are standardised through the calculation of z-scores, which normalise the data by its mean and standard deviation, ensuring comparability across different scales. Categorical columns, on the other hand, are transformed using one-hot encoding, a technique that converts categorical variables into a form that could be provided to machine learning algorithms for better prediction. 
Finally, we focus on consolidating our pre-processed data. We concatenate the numeric and categorical datasets. For the one-hot encoded columns, we also compute z-scores, similar to the numeric columns, to maintain consistency in normalisation across the dataset. This step ensures that all features, regardless of their original format, are standardised and ready for advanced analysis. The resultant z-score normalised data frames, both numeric and categorical, are then concatenated to form a comprehensive, normalised dataset. This meticulous preparation renders the data primed for robust and accurate analytical modelling.

### Exploratory Analysis
Upon thorough review of our cleaned data set, we uncovered that the mean two-year cohort default rate sits around 8.4%, but it peaks at an alarming 50% for some institutions. A deeper dive using the groupby function exposed a trend: private for-profit institutions and those whose highest award degree is associate degrees consistently register higher default rates. In a geographical context, educational institutions located in Nevada manifest a notably higher default risk.

Correlation analysis between various features and the default rate brought to light some insightful patterns. The variable HIGHDEG_2, indicative of schools where the highest degree conferred is an associate's, has the highest positive correlation with the default rate. This suggests that schools offering up to associate degrees may not be providing sufficient career and earning prospects to aid graduates in loan repayment.

On the flip side, MD_EARN_WNE_MALE0_P8, capturing the median earnings of female graduates who are employed and not enrolled in further studies after eight years, showed a strong negative correlation with the default rate. The higher median earnings are indicative of better financial well-being among graduates, which in turn correlates with a lower propensity for loan default.

![Correlation](https://github.com/KrystalSsong/QTM-347-Final-Project/assets/63956791/6d769d28-9e7e-44da-94e6-80fc53878ffb)

## Model Selection
### Feature Selection
Faced with a dataset of over 300 features, we tried three main models for feature selection to distill our extensive feature set into a more manageable and significant subset, which is crucial for developing robust predictive models. We started with a decision tree regressor to identify key variables through its feature importance ranking. We also applied forward selection to incrementally build a model, focusing on the most impactful features. Besides, we also used Lasso regression, known for its effectiveness in reducing feature redundancy by shrinking less important feature coefficients to zero. After comparing MSE and features selected, we decided to use features selected by Lasso. 

### Regression
We then employed a decision tree and random forest regressor to predict the cohort default rate due to their effectiveness in handling complex, high-dimensional data. The Decision Tree regressor provides a straightforward, interpretable model, making it easier to understand how each feature influences the default rate. However, it can be prone to overfitting, especially with a large number of features. To counter this, we employed the Random Forest regressor, an ensemble method that aggregates multiple decision trees to improve prediction accuracy and robustness. Random Forest is particularly adept at managing overfitting while still handling a large feature set effectively. This combination of models allowed us to capture the nuances of our extensive dataset and generate reliable predictions of default rates.

### Classification
To frame our investigation as a business problem, we transformed it into a classification task, focusing on determining if an institution poses a high risk (1) or not (0), based on its cohort default rate. We set the risk threshold at 10%, guided by the Holders in Due Course principle. This threshold choice is also substantiated by the default rate distribution in our dataset, where about half of the data records a default rate around 8%. Such a threshold ensures a balanced distribution between the two classes (high risk and low risk), thereby obviating the need for techniques like over- or under-sampling that are typically employed to address class imbalance. This approach allows for a more straightforward and effective application of classification algorithms in predicting institutional risk. We would like to use decision tree and random forest classifier here. 

## Results
### Feature Selection
For feature selection, we initially experimented with a decision tree regressor, adjusting the maximum depth to prevent overfitting. However, the optimal model identified had minimal depth, suggesting that among the extensive set of features, only a few significantly contribute to predictive accuracy.

Our analysis proceeded under two hypotheses regarding the nature of the predictive model:
* The true model is sparse, with only a select few features being relevant — indicating the potential effectiveness of lasso regression.
* The true model relies on numerous interrelated features, suggesting that principal component regression (PCR) might be appropriate.

We selected a lambda (0.002) smaller than the optimal value (0.0004), as the optimal lambda applied limited penalization on features, not sufficiently reducing their number. Consequently, we opted for a larger lambda. This adjustment did not significantly alter the train and test Mean Squared Error (MSE). Following this, our lasso regression identified about 40 relevant features. Similarly, forward selection also highlighted a similar number of significant features. Further increasing the lambda in the lasso regression and restricting forward selection to only four features resulted in consistent findings across both methods.
![Lasso_coefficients](https://github.com/KrystalSsong/QTM-347-Final-Project/assets/63956791/f67d4c56-0819-4b08-916d-ee0c77f614fd)

Comparative analysis revealed that both PCR and PLS produced higher mean squared errors in training and testing phases than the lasso regression. Given the lack of transparency in how PCR and PLS components contribute to the model, we opted to proceed with the feature set identified by lasso regression for its predictive efficiency and interpretability.

### Regression
We developed two predictive regression models: a Decision Tree Regressor and a Random Forest Regressor. For the Decision Tree model, the optimal complexity level was determined to be 3.2009e-05, resulting in a training Mean Squared Error (MSE) of 0.002632 and a test MSE of 0.002515. The low MSE values achieved by the Decision Tree Regressor indicate its strong performance. Furthermore, the slight increase in MSE from the training to the test set suggests effective generalization. This optimal level of complexity reflects a balanced model that successfully avoids overfitting while accurately capturing the key patterns in the data.

![DTR](https://github.com/KrystalSsong/QTM-347-Final-Project/assets/63956791/de536c67-ceef-4f51-98f6-aaadb2f9e19e)

The Random Forest Regressor functions by building numerous decision trees during training and delivering an average of these trees' predictions, yielding a more precise and stable forecast than a single decision tree. We determined the optimal configuration to include 237 trees and to consider 9 features at each split. This model achieved a remarkably low training Mean Squared Error (MSE) of 0.000323 and a test MSE of 0.002172. Such a low training MSE is characteristic of Random Forests, as they typically diminish variance through the averaging process. Notably, the test MSE is lower than that of the Decision Tree Regressor, signifying superior generalization capabilities when applied to new, unseen data. This underscores the Random Forest Regressor's robustness and effectiveness in predictive modeling.

### Classification
To more easily calculate the economic scale of the models, we shift to classification problems of predicting binary outcomes. We set the threshold as 0.1 because of the Holders in Due Course. Also, according to the distribution of default rate, 0.1 will make two classes equally distributed so we don’t need to handle oversampling or undersampling.

The first model we used is the logistic regression to predict the probability of default rate > 0.1. We got a training accuracy equal to 0.757782 and a test accuracy equal to 0.732365. 

Our second model, decision tree, is powerful for modelling complex, non-linear relationships. The optimal depth is 6 with the training accuracy of 0.796917 and the test accuracy of 0.751037. The Decision Tree model shows a tendency to overfit since the training accuracy is significantly higher than the test accuracy. Despite the overfitting, it still outperforms the Logistic Regression on the training set due to its non-linear nature, but it slightly underperforms on the test set. 
![DTC](https://github.com/KrystalSsong/QTM-347-Final-Project/assets/63956791/35ab1968-0b67-437e-80aa-c9bdfe9508e6)

The last model, Random Forest Classifier, is particularly well-suited for our situation, since we have large datasets with relatively high dimensionality. The optimal number of trees is 162, and the optimal max_features is 5. The training accuracy is 0.832790, and the test accuracy is 0.781466. The Random Forest outperforms both the Logistic Regression and the Decision Tree in terms of accuracy on the test set. This indicates that the ensemble method, which builds upon the Decision Tree model, is more effective at generalising from the training data to unseen data.

The confusion matrix presented compares the performance of a Decision Tree classifier and a Random Forest classifier on a test sample of 1446 issued loans. 
![confusion_matrix](https://github.com/KrystalSsong/QTM-347-Final-Project/assets/63956791/5bece94a-d9a4-489a-91bf-6e7bfbea4468)
* Accuracy: Both models are reasonably accurate in predicting 'Non-Default', but the Random Forest has a higher number of True Positives (306 vs. 256), which suggests it is better at correctly identifying 'Default' cases.
* Precision: Precision (the number of True Positives divided by the number of True Positives and False Positives) is higher for the Decision Tree for predicting 'Default' (256/(256+115)) compared to the Random Forest (306/(306+121)). However, this difference is small.
* Recall: Recall (the number of True Positives divided by the number of True Positives and False Negatives) is notably higher for the Random Forest (306/(306+195)) compared to the Decision Tree (256/(256+245)), indicating the Random Forest is less likely to miss 'Default' cases.
* False Positives and False Negatives: The Random Forest has slightly fewer False Positives but significantly fewer False Negatives, indicating it is more conservative about predicting 'Default' and is better at identifying actual 'Default' cases.

Therefore, given the cost assumptions, the Random Forest model is more cost-effective due to its lower number of False Positives, despite both models having the same number of True Positives and False Negatives.

## Limitations
Our study faces several limitations that may impact the comprehensiveness and applicability of its findings. One of the key constraints is the lack of economic factors such as rent prices and home prices, which can be important variables influencing post-graduation income and employment. These factors directly reflect the students’ abilities to repay their debt. Furthermore, dropping numerous variables due to excessive missing data (NAs) potentially omits critical information that could have provided a more holistic understanding of the students' economic outcomes. Additionally, the dataset's structure, centred around institutions rather than individual students, limits the granularity of the analysis, potentially overlooking student-specific factors that affect their economic prospects. A significant limitation also arises from the dataset's exclusive inclusion of students who were approved for loans, omitting those who were denied. This exclusion could skew the results, as the financial backgrounds and subsequent career trajectories of students who do not receive loans might differ substantially from those who do. Overall, these limitations suggest that while the study can offer valuable insights, its findings should be interpreted with an understanding of these constraints and the potential for unaccounted variables impacting the outcomes.

## Discussion
### Who defaults on student loans? Findings from the National Postsecondary Student Aid Study - ScienceDirect
The analysis of the National Postsecondary Student Aid Study Supplement Loan Recipients Survey (NPSASSLRS) data presents several key findings regarding student loan defaults. The study reveals that borrowers from low-income households, minority groups, high school dropouts, those who do not complete their postsecondary programs, and attendees of proprietary schools and two-year colleges are more prone to defaulting on their loan payments. 

### Project MUSE - College on Credit: A Multilevel Analysis of Student Loan Default
The study provides a comprehensive analysis of the factors contributing to student loan defaults, emphasizing the significant role of institutional context. It finds that students attending for-profit colleges are more likely to default on their loans compared to those in other higher education sectors, even after controlling for various individual characteristics. Factors such as the type of institution attended, degree completion, and post-college employment are more crucial in determining default status. Individual-level characteristics also play a role; students from upper-income families or non-first-generation students have lower odds of defaulting. Conversely, lower-income students, minoritized students, and students with dependents face higher default risks.

Although these research studies' used datasets are based on individual students instead of by institutions like our dataset, we all reached an agreement in terms of important factors that impact student loan default rates.

### Data Ethics
We chose a lasso regression model with a reduced feature set, partly guided by considerations of data ethics. While race emerged as a significant predictor, we consciously avoided using it to prevent our model from perpetuating racial biases. It's essential to ensure that minorities are not disadvantaged in educational opportunities, such as receiving student loans and accessing college education. Additionally, historical data often carry inherent biases, and a more regularized model, like lasso regression, helps in generalizing the model better by focusing on less discriminatory and more relevant predictors. This approach aligns with our commitment to ethical data practices and ensures our model contributes to fair and equitable educational financing.

## Conclusions
In conclusion, our analysis has revealed several key features that significantly influence student loan default rates. The type of institution emerges as a critical factor, with private nonprofit institutions showing the highest default rates. Majors also play a pivotal role, with fields like homeland security, firefighting, and humanities being most likely to default. The highest degree achieved, particularly an Associate’s degree, correlates with default tendencies. There is also significant correlation between median incomes after graduation and default rates, with the incomes of female graduates being particularly indicative of an institution's default rates. Furthermore, student race is a significant determinant, with African American students showing a higher likelihood of defaulting on loans, while Caucasian students are comparatively less likely to default. These findings underscore the complex interplay of educational, demographic, and economic factors in student loan defaults, offering valuable insights for policy makers and educational institutions in mitigating financial risks and supporting student success.

## References
Dynarski, M. (1994). Who defaults on student loans? Findings from the National Postsecondary Student Aid Study. Economics of Education Review, 13(1), 55-68. ISSN 0272-7757. https://doi.org/10.1016/0272-7757(94)90023-X. Available at ScienceDirect.

Hillman, Nicholas W. "College on Credit: A Multilevel Analysis of Student Loan Default." The Review of Higher Education, vol. 37 no. 2, 2014, p. 169-195. Project MUSE, https://doi.org/10.1353/rhe.2014.0011.


